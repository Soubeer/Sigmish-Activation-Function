# -*- coding: utf-8 -*-
"""custom activation function.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1KBN-neIa2NrXxHZuoGiJhlo0HP1mIGfM
"""

import torch
import torch.nn as nn
import torch.nn.functional as F

# Custom Sigmish activation function
class SigmishActivation(nn.Module):
    def __init__(self):
        super(SigmishActivation, self).__init__()
        #learnable parameters k0, k1, k2
        self.k0 = nn.Parameter(torch.tensor(0.5, dtype=torch.float32))
        self.k1 = nn.Parameter(torch.tensor(0.5, dtype=torch.float32))
        self.k2 = nn.Parameter(torch.tensor(0.5, dtype=torch.float32))

    def forward(self, x):
        sigmoid_part = self.k0 * torch.sigmoid(x)
        mish_part = self.k1 * x * torch.tanh(torch.log1p(torch.exp(x)))
        sin_part = self.k2 * torch.sin(x)
        return sigmoid_part + mish_part + sin_part

# Example
class CustomNet(nn.Module):
    def __init__(self):
        super(CustomNet, self).__init__()
        self.fc1 = nn.Linear(784, 128)
        self.activation = SigmishActivation()  # Custom activation layer
        self.fc2 = nn.Linear(128, 10)

    def forward(self, x):
        x = self.fc1(x)
        x = self.activation(x)
        x = self.fc2(x)
        return F.log_softmax(x, dim=1)  # Output layer(classification)

# Instantiating model
model = CustomNet()

# Sample input
sample_input = torch.randn(1, 784)
output = model(sample_input)
print(output)